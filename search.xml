<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>深入认识 Scrapy框架中settings文件</title>
      <link href="2021/04/30/shen-ru-ren-shi-scrapy-zhong-settings-wen-jian/"/>
      <url>2021/04/30/shen-ru-ren-shi-scrapy-zhong-settings-wen-jian/</url>
      
        <content type="html"><![CDATA[<h2 id="深入认识-Scrapy框架中settings文件"><a href="#深入认识-Scrapy框架中settings文件" class="headerlink" title="深入认识 Scrapy框架中settings文件"></a>深入认识 Scrapy框架中settings文件</h2><h3 id="为什么Scrapy项目中会存在settings文件呢，他到底是什么？"><a href="#为什么Scrapy项目中会存在settings文件呢，他到底是什么？" class="headerlink" title="为什么Scrapy项目中会存在settings文件呢，他到底是什么？"></a>为什么Scrapy项目中会存在settings文件呢，他到底是什么？</h3><ul><li>配置文件存放一些公共的变量（比如数据库的地址、账号+密码等），方便自己和别人修改</li><li>一般使用全大写字母命名变量名，例如 <code>LOG_LEVEL=&quot;WARN&quot;</code></li></ul><h2 id="settings文件详细解读"><a href="#settings文件详细解读" class="headerlink" title="settings文件详细解读"></a>settings文件详细解读</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">BOT_NAME <span class="token operator">=</span> <span class="token string">'Sun'</span>  <span class="token comment"># 项目名</span>SPIDER_MODULES <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'Sun.spiders'</span><span class="token punctuation">]</span>  <span class="token comment"># 爬虫的位置</span>NEWSPIDER_MODULE <span class="token operator">=</span> <span class="token string">'Sun.spiders'</span>  <span class="token comment"># 如果我们新建一个爬虫，这个新建爬虫的位置</span>LOG_LEVEL<span class="token operator">=</span><span class="token string">"WARN"</span>  <span class="token comment"># log日志显示的最低等级，只有等级大于或等于LOG_LEVEL的才会在终端中显示</span><span class="token comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span>USER_AGENT <span class="token operator">=</span> <span class="token string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36"</span><span class="token comment"># 浏览器的身份标识，即是"User-Agent"</span><span class="token comment"># Obey robots.txt rules</span>ROBOTSTXT_OBEY <span class="token operator">=</span> <span class="token boolean">True</span><span class="token comment"># 默认情况下为True，表示遵守这个网站的robots协议</span><span class="token comment"># 如果我们设置</span><span class="token comment"># ROBOTSTXT_OBEY = True  # 不遵守robots协议</span><span class="token comment"># 遵守robots协议的体现：最开始请求url地址的时候会先请求对应的robots.txt，如果我们不遵守robots协议，就不会请求</span><span class="token comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span><span class="token comment">#CONCURRENT_REQUESTS = 32  # 设置最大并发请求</span><span class="token comment"># scrapy中封装了一个twisted异步网络框架，在发送网络请求的时候不会说等一个请求请求成功之后再去发送下一个请求，而是同时去发送很多个请求</span><span class="token comment"># 大概有多少个请求可以同时被发送呢？我们可以在这里自定义数量</span><span class="token comment"># 注意：默认是16个，我们可以根据自己的爬虫项目需要自定义CONCURRENT_REQUESTS的数量</span><span class="token comment"># CONCURRENT_REQUESTS的值越大，爬虫爬取的速度越快</span><span class="token comment"># 但是注意，CONCURRENT_REQUESTS的值越大，越有可能被对方的服务器识别为一个爬虫程序</span><span class="token comment"># 所以说，要适当设置CONCURRENT_REQUESTS的数量</span><span class="token comment">#DOWNLOAD_DELAY = 3  # 下载延迟</span><span class="token comment"># 每次请求之前，休眠3秒</span><span class="token comment"># 可以让我们的爬虫请求速度变慢一些</span><span class="token comment"># The download delay setting will honor only one of:</span><span class="token comment"># 就是说我们的DOWNLOAD_DELAY参数可以配合着下面的CONCURRENT_REQUESTS_PER_DOMAIN参数和CONCURRENT_REQUESTS_PER_IP参数来使用</span><span class="token comment">#CONCURRENT_REQUESTS_PER_DOMAIN = 16  # 每一个域名的最大并发请求数目</span><span class="token comment">#CONCURRENT_REQUESTS_PER_IP = 16  # 每一个IP的最大并发请求数目</span><span class="token comment"># Disable cookies (enabled by default)</span><span class="token comment">#COOKIES_ENABLED = False  # cookie是否开启</span><span class="token comment"># 默认情况下是开启的，也就是说我们请求完一个url地址之后，去请求下一个url地址的时候，scrapy默认是带上前一次请求对方服务器设置在我们本地的cookie信息</span><span class="token comment"># 很重要</span><span class="token comment"># Disable Telnet Console (enabled by default)</span><span class="token comment">#TELNETCONSOLE_ENABLED = False</span><span class="token comment"># 这是我们scrapy框架的一个插件</span><span class="token comment"># 默认是开启的状态</span><span class="token comment"># Override the default request headers:</span><span class="token comment">#DEFAULT_REQUEST_HEADERS = &#123;</span><span class="token comment">#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',</span><span class="token comment">#   'Accept-Language': 'en',</span><span class="token comment">#&#125;</span><span class="token comment"># 默认的请求头，默认是关闭的状态</span><span class="token comment"># 开启之后，scrapy就会用这个请求头去发送请求</span><span class="token comment"># 注意：如果我们将"User-Agent"参数放在这里，是没有任何效果的，因为专门有一个参数接收"User-Agent"对应的值</span><span class="token comment"># Enable or disable spider middlewares</span><span class="token comment"># See https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><span class="token comment"># SPIDER_MIDDLEWARES = &#123;</span><span class="token comment">#    'Sun.middlewares.SunSpiderMiddleware': 543,</span><span class="token comment"># &#125;</span><span class="token comment"># 爬虫中间件</span><span class="token comment"># Enable or disable downloader middlewares</span><span class="token comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><span class="token comment">#DOWNLOADER_MIDDLEWARES = &#123;</span><span class="token comment">#    'Sun.middlewares.SunDownloaderMiddleware': 543,</span><span class="token comment">#&#125;</span><span class="token comment"># 下载中间件</span><span class="token comment"># Enable or disable extensions</span><span class="token comment"># See https://docs.scrapy.org/en/latest/topics/extensions.html</span><span class="token comment">#EXTENSIONS = &#123;</span><span class="token comment">#    'scrapy.extensions.telnet.TelnetConsole': None,</span><span class="token comment">#&#125;</span><span class="token comment"># 插件</span><span class="token comment"># Configure item pipelines</span><span class="token comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>ITEM_PIPELINES <span class="token operator">=</span> <span class="token punctuation">&#123;</span>   <span class="token string">'Sun.pipelines.SunPipeline'</span><span class="token punctuation">:</span> <span class="token number">300</span><span class="token punctuation">,</span><span class="token punctuation">&#125;</span><span class="token comment"># 管道</span><span class="token comment"># 爬虫中间件、下载中间件、插件、管道的使用方法：</span><span class="token comment"># 类似于ITEM_PIPELINES管道的使用方法</span><span class="token comment"># 开启之后，键--位置，值--和引擎之间的距离（注意：值越大，权重越低；值越小，权重越高）</span><span class="token comment"># Enable and configure the AutoThrottle extension (disabled by default)</span><span class="token comment"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html</span><span class="token comment">#AUTOTHROTTLE_ENABLED = True</span><span class="token comment"># AUTOTHROTTLE--自动限速</span><span class="token comment"># 比如说我们通过scrapy爬取对方的网站，可能会由于我们的scrapy爬虫速度太快导致把对方的网站“抓崩了”</span><span class="token comment"># “抓崩了”并不是我们想要的结果，因为“抓崩了”之后我们就没有办法再在对方的网站上爬取数据了</span><span class="token comment"># 我们可以根据对方网站的情况让我们的scrapy爬虫速度变慢一些</span><span class="token comment"># 通过调节下面的参数我们可以达到这个自动限速的目的</span><span class="token comment"># 但是往往我们是不需要这样操作的</span><span class="token comment"># The initial download delay</span><span class="token comment">#AUTOTHROTTLE_START_DELAY = 5</span><span class="token comment"># The maximum download delay to be set in case of high latencies</span><span class="token comment">#AUTOTHROTTLE_MAX_DELAY = 60</span><span class="token comment"># The average number of requests Scrapy should be sending in parallel to</span><span class="token comment"># each remote server</span><span class="token comment">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span><span class="token comment"># Enable showing throttling stats for every response received:</span><span class="token comment">#AUTOTHROTTLE_DEBUG = False</span><span class="token comment"># Enable and configure HTTP caching (disabled by default)</span><span class="token comment"># 关于HTTP的缓存，默认是关闭的</span><span class="token comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span><span class="token comment">#HTTPCACHE_ENABLED = True</span><span class="token comment">#HTTPCACHE_EXPIRATION_SECS = 0</span><span class="token comment">#HTTPCACHE_DIR = 'httpcache'</span><span class="token comment">#HTTPCACHE_IGNORE_HTTP_CODES = []</span><span class="token comment">#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'</span><span class="token comment"># 如果我们想要开启HTTP缓存就可以将其打开</span><span class="token comment"># 它会将我们的HTTP缓存至设置的位置，实现对应的功能</span><span class="token comment"># 不常用</span><span class="token comment"># 每一块内容都是有对应的官方文档的</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="如何在代码中使用我们在settings-py文件中所做的设置"><a href="#如何在代码中使用我们在settings-py文件中所做的设置" class="headerlink" title="如何在代码中使用我们在settings.py文件中所做的设置"></a><strong>如何在代码中使用我们在settings.py文件中所做的设置</strong></h2><p>现在，我们在<code>settings.py</code>文件中做了一些设置</p><p><a href="https://www.ipicbed.com/image/n4mz"><img src="https://www.ipicbed.com/images/2021/04/30/1.md.png" alt=" "></a></p><p><code>settings.py</code>文件中有一个<code>MONGO_HOST</code>，我们应该如何在spider中使用呢？</p><h3 id="第一种方法：导入"><a href="#第一种方法：导入" class="headerlink" title="第一种方法：导入"></a>第一种方法：导入</h3><p>1.导入</p><p><a href="https://www.ipicbed.com/image/n8n8"><img src="https://www.ipicbed.com/images/2021/04/30/2.md.png" alt=" "></a></p><p>2.现在我们就可以使用我们在<code>settings.py</code>文件中定义的这个<code>MONGO_HOST</code>参数了</p><p>3.如果我们想要在其他文件中使用<code>MONGO_HOST</code>这个参数也是一样方法：导入</p><p>以<code> pipelines.py</code> 为例，</p><p><code>from .settings import MONGO_HOST</code></p><p><a href="https://www.ipicbed.com/image/ntWC"><img src="https://www.ipicbed.com/images/2021/04/30/3.md.png" alt=" "></a></p><h3 id="第二种方法：属性"><a href="#第二种方法：属性" class="headerlink" title="第二种方法：属性"></a>第二种方法：属性</h3><p>其实我们的spider本身就有一个属性就是settings</p><h4 id="1-如何在爬虫中使用settings属性"><a href="#1-如何在爬虫中使用settings属性" class="headerlink" title="1.如何在爬虫中使用settings属性"></a><strong>1.如何在爬虫中使用settings属性</strong></h4><p><a href="https://www.ipicbed.com/image/nYJE"><img src="https://www.ipicbed.com/images/2021/04/30/4.md.png" alt=" "></a></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># -*- coding: utf-8 -*-</span><span class="token keyword">import</span> scrapy<span class="token keyword">from</span> <span class="token punctuation">.</span><span class="token punctuation">.</span>items <span class="token keyword">import</span> SunItem<span class="token comment"># from ..settings import MONGO_HOST</span><span class="token keyword">class</span> <span class="token class-name">SunSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>    name <span class="token operator">=</span> <span class="token string">'sun'</span>    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'sun0769.com'</span><span class="token punctuation">]</span>    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'http://wz.sun0769.com/political/index/politicsNewest?id=1&amp;page=1'</span><span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># # spider本身就有一个属性叫做settings</span>        <span class="token comment"># self.settings["MONGO_HOST"]  # self.settings是一个字典</span>        <span class="token comment"># self.settings.get("MONGO_HOST","指定的MONGO_HOST的值")</span>        <span class="token triple-quoted-string string">"""        由于self.settings是一个字典。所以我们可以对其使用        self.settings.get("MONGO_HOST")  # 如果MONGO_HOST不存在，返回的就是一个None值        # 或者是返回我们在get方法中给MONGO_HOST手动指定的值        self.settings.get("MONGO_HOST","指定的MONGO_HOST的值")        """</span>        <span class="token comment"># 分组</span>        li_list<span class="token operator">=</span>response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//div[@class="width-12"]/ul[@class="title-state-ul"]//li[@class="clear"]'</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> li <span class="token keyword">in</span> li_list<span class="token punctuation">:</span>            item<span class="token operator">=</span>SunItem<span class="token punctuation">(</span><span class="token punctuation">)</span>            item<span class="token punctuation">[</span><span class="token string">"num"</span><span class="token punctuation">]</span><span class="token operator">=</span>li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'.//span[@class="state1"]/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>            item<span class="token punctuation">[</span><span class="token string">"title"</span><span class="token punctuation">]</span><span class="token operator">=</span>li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'.//span[@class="state3"]/a[1]/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>            item<span class="token punctuation">[</span><span class="token string">"response_time"</span><span class="token punctuation">]</span><span class="token operator">=</span>li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'.//span[@class="state4"]/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>            item<span class="token punctuation">[</span><span class="token string">"response_time"</span><span class="token punctuation">]</span><span class="token operator">=</span>item<span class="token punctuation">[</span><span class="token string">"response_time"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"："</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>            item<span class="token punctuation">[</span><span class="token string">"ask_time"</span><span class="token punctuation">]</span><span class="token operator">=</span>li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'.//span[@class="state5 "]/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>            item<span class="token punctuation">[</span><span class="token string">"detail_url"</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">"http://wz.sun0769.com"</span><span class="token operator">+</span>li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'.//span[@class="state3"]/a[1]/@href'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>item<span class="token punctuation">[</span><span class="token string">"detail_url"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                 callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse_detail_url<span class="token punctuation">,</span>                                 meta<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">"item"</span><span class="token punctuation">:</span>item<span class="token punctuation">&#125;</span>  <span class="token comment"># 通过meta传递数据</span>                                 <span class="token punctuation">)</span>        <span class="token comment"># 实现翻页操作</span>        <span class="token keyword">for</span> page <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            next_url <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"http://wz.sun0769.com/political/index/politicsNewest?id=1&amp;page=</span><span class="token interpolation"><span class="token punctuation">&#123;</span>page<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>            <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>next_url<span class="token punctuation">,</span>                                 callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse                                 <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">parse_detail_url</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>response<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""处理详情页的数据"""</span>        item<span class="token operator">=</span>response<span class="token punctuation">.</span>meta<span class="token punctuation">[</span><span class="token string">"item"</span><span class="token punctuation">]</span>  <span class="token comment"># 取出数据</span>        <span class="token comment"># 注意：这样获得的内容是混杂的，包含了很多的占位符等垃圾符号</span>        item<span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span><span class="token operator">=</span>response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//div[@class="details-box"]/pre/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 注意：图片和视频可能不止一个，也可能没有</span>        item<span class="token punctuation">[</span><span class="token string">"img"</span><span class="token punctuation">]</span><span class="token operator">=</span>response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//div[@class="clear details-img-list Picture-img"]/img/@src'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>        item<span class="token punctuation">[</span><span class="token string">"video"</span><span class="token punctuation">]</span><span class="token operator">=</span>response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//div[@class="vcp-player"]/video/@src'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">yield</span> item<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-如何在其他文件中使用settings属性"><a href="#2-如何在其他文件中使用settings属性" class="headerlink" title="2.如何在其他文件中使用settings属性"></a><strong>2.如何在其他文件中使用settings属性</strong></h4><blockquote><p><em>以pipelines.py为例</em></p><p><a href="https://www.ipicbed.com/image/nf3N"><img src="https://www.ipicbed.com/images/2021/04/30/5.md.png" alt=" "></a></p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># -*- coding: utf-8 -*-</span><span class="token comment"># Define your item pipelines here</span><span class="token comment">#</span><span class="token comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span><span class="token comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><span class="token keyword">import</span> logging<span class="token comment"># from .settings import MONGO_HOST</span>logger<span class="token operator">=</span>logging<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span>__name__<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">SunPipeline</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># spider.settings</span>        <span class="token comment"># spider.settings就是self.settings</span>        <span class="token triple-quoted-string string">"""        spider.settings["MONGO_HOST"]        spider.settings.get("MONGO_HOST","指定的MONGO_HOST的值")        """</span>        item<span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span><span class="token operator">=</span>self<span class="token punctuation">.</span>process_content<span class="token punctuation">(</span>item<span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment"># logger.warning(item)</span>        <span class="token keyword">return</span> item    <span class="token keyword">def</span> <span class="token function">process_content</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>content<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""处理item中的字符串"""</span>        new_content<span class="token operator">=</span>content<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"\r\n"</span><span class="token punctuation">,</span><span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"\xa0"</span><span class="token punctuation">,</span><span class="token string">""</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> new_content<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>scrapy深入之pipeline的使用</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">JsonWriterPipeline</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">open</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>spider<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""在爬虫开启的时候执行，仅执行一次"""</span>        self<span class="token punctuation">.</span><span class="token builtin">file</span><span class="token operator">=</span><span class="token builtin">open</span><span class="token punctuation">(</span>spider<span class="token punctuation">.</span>setting<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"SAVE_FILE"</span><span class="token punctuation">,</span><span class="token string">"./temp.json"</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">"w"</span><span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">close</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>spider<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""在爬虫关闭的时候执行，仅执行一次"""</span>        self<span class="token punctuation">.</span><span class="token builtin">file</span><span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>item<span class="token punctuation">,</span>spider<span class="token punctuation">)</span><span class="token punctuation">:</span>        line<span class="token operator">=</span>json<span class="token punctuation">.</span>dumps<span class="token punctuation">(</span><span class="token builtin">dict</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">"\n"</span>  <span class="token comment"># 将字典转化为json字符串</span>        self<span class="token punctuation">.</span><span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span>line<span class="token punctuation">)</span>        <span class="token keyword">return</span> item  <span class="token comment"># 如果不return的情况下，另一个权重较低的pipeline就不会获取到该item</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>open_spider`在爬虫开启的时候执行一次</li><li><code>close_spider</code>在爬虫关闭的时候执行一次</li></ul><p><strong>好处1：数据库的处理：</strong></p><ul><li>建立连接的过程就可以放在<code>open_spider</code>中</li><li>断开连接的过程就放在<code>close_spider</code>中</li></ul><p><strong>好处2：在爬虫开始时为爬虫添加一些属性</strong></p><ol><li><p>在爬虫开始的时候为爬虫添加了一个新的属性<code>hello</code>，值为<code>&quot;world&quot;</code></p><p>位置：<code>pipelines.SunPipeline</code></p></li></ol><p><a href="https://www.ipicbed.com/image/nQfu"><img src="https://www.ipicbed.com/images/2021/04/30/6.md.png" alt=" "></a></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">open_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>spider<span class="token punctuation">)</span><span class="token punctuation">:</span>        spider<span class="token punctuation">.</span>hello<span class="token operator">=</span><span class="token string">"world"</span>  <span class="token comment"># 为spider创建一个新的属性叫做"hello"，值为"world"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol start="2"><li>在爬虫中调用这个属性</li></ol><p><a href="https://www.ipicbed.com/image/npoT"><img src="https://www.ipicbed.com/images/2021/04/30/7.md.png" alt=" "></a></p><p>注意：现在我们至是想要看见<code>scrapy</code>在爬虫启动的时候是否执行了open_spider这个方法，不需要爬取网站的信息，所以我们修改爬取的范围，即是 <code>allowed_domains</code></p><ol start="2"><li>启动我们的爬虫程序</li></ol><p><code>scrapy crawl sun</code></p><p><a href="https://www.ipicbed.com/image/nq1b"><img src="https://www.ipicbed.com/images/2021/04/30/8.md.png" alt=" "></a></p><p>发现确实是输出了<code>hello</code>属性对应的值，说明我们的爬虫在开始的时候的确是调用了open_spider这个函数</p><h2 id="pipeline中对数据库的处理"><a href="#pipeline中对数据库的处理" class="headerlink" title="pipeline中对数据库的处理"></a><strong>pipeline中对数据库的处理</strong></h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 对数据库进行处理</span><span class="token keyword">from</span> pymongo <span class="token keyword">import</span> MongoClient<span class="token keyword">class</span> <span class="token class-name">SunPipeline</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">open_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>spider<span class="token punctuation">)</span><span class="token punctuation">:</span>        client<span class="token operator">=</span>MongoClient<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 实例化一下client</span>        self<span class="token punctuation">.</span>collection<span class="token operator">=</span>client<span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 调用self.collection，直接insert</span>        self<span class="token punctuation">.</span>collection<span class="token punctuation">.</span>insert<span class="token punctuation">(</span><span class="token builtin">dict</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>当我们需要把文件写在本地的时候</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">JsonWriterPipeline</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">open</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>spider<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""在爬虫开启的时候执行，仅执行一次"""</span>        self<span class="token punctuation">.</span><span class="token builtin">file</span><span class="token operator">=</span><span class="token builtin">open</span><span class="token punctuation">(</span>spider<span class="token punctuation">.</span>setting<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"SAVE_FILE"</span><span class="token punctuation">,</span><span class="token string">"./temp.json"</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">"w"</span><span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">close</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>spider<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""在爬虫关闭的时候执行，仅执行一次"""</span>        self<span class="token punctuation">.</span><span class="token builtin">file</span><span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>item<span class="token punctuation">,</span>spider<span class="token punctuation">)</span><span class="token punctuation">:</span>        line<span class="token operator">=</span>json<span class="token punctuation">.</span>dumps<span class="token punctuation">(</span><span class="token builtin">dict</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">"\n"</span>  <span class="token comment"># 将字典转化为json字符串</span>        self<span class="token punctuation">.</span><span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span>line<span class="token punctuation">)</span>        <span class="token keyword">return</span> item  <span class="token comment"># 如果不return的情况下，另一个权重较低的pipeline就不会获取到该item</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>思路：</strong></p><ul><li>在<code>open_spider</code>中打开一次</li><li>在<code>close_spider</code>中关闭一次</li><li>中间不停地往文件中写入数据</li></ul><p>这种方式是可以的，但是会有问题</p><p><strong>问题：</strong></p><p>如果我们的爬虫在爬取网站的时候爬取到第3页的时候程序突然报错了，然后爬虫终止执行，这时候，前2页的数据能够保存在我们的文件中吗？</p><p><strong>不会</strong></p><p><strong>为什么？</strong></p><p>因为我们的文件打开之后写入数据只有在close之后数据才会写进去，如果我们不close的话，数据是不会写入的。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scrapy </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
